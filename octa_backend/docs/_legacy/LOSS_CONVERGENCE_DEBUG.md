# Loss收敛停滞诊断指南

当loss卡在0.6左右无法进一步下降时，使用本指南逐步排查根本原因。

---

## 📊 诊断信息解读

### 1. **Loss平台期检查**（每10个epoch输出一次）

```
[诊断] Loss平台期检查:
  过去5个epoch loss: ['0.612534', '0.611893', '0.612105', '0.611567', '0.612001']
  方差: 8.23e-07, 5-epoch改进: 0.000533
  ⚠️ Loss已进入平台期（方差<1e-6, 改进<0.0001）
```

**解读：**
- **方差 < 1e-6** → loss几乎不变动，说明优化已停滞
- **5-epoch改进 < 0.0001** → 即使有改进也极其微小
- **结论** → 模型无法从当前点继续学习

---

### 2. **模型输出诊断**（每5个epoch）

```
[诊断] Epoch 15 模型输出:
  原始输出 range: [-0.2543, 0.1234]
  Sigmoid输出 range: [0.4367, 0.5309]
  Sigmoid均值: 0.4823
  掩码 range: [0.0000, 1.0000]
  掩码均值: 0.1234 (前景比例: 0.1234)
  Loss 分量: Total=0.612345, BCE=0.234567, Dice=0.377778
```

**关键指标：**

| 指标 | 正常范围 | 异常信号 | 解决方案 |
|-----|--------|--------|--------|
| **Sigmoid均值** | 0.1-0.9 | 接近0.5 | 模型陷入不确定，梯度信号弱 |
| **Sigmoid range** | [0,1] | [0.4, 0.6] | 输出分化度低，模型学不动 |
| **前景比例** | 5%-50% | 很低(<2%) | 严重类不平衡，需要权重调整 |
| **Dice loss** | <0.2 | >0.35 | Dice主导，模型未学会分割形状 |
| **BCE loss** | <0.2 | >0.3 | BCE主导，像素级分类还未稳定 |

---

### 3. **梯度范数诊断**（每个batch）

```
Epoch [25] Step 8 | Layer encoder_conv1: 0.052314 | encoder_last: 0.003101 | 
decoder_first: 0.004522 | decoder_output: 0.002881 | 
Hint: ⚠️浅层/深层>10x，轻微消失
```

**判定规则：**
- **所有层 > 1e-4** → ✓梯度健康，正常学习
- **某层 < 1e-6** → ⚠️该层梯度消失，需要检查BN/激活
- **浅层/深层 > 10x** → ⚠️轻微梯度消失，导致深层学习缓慢
- **所有层 < 1e-6** → 🔴严重梯度消失，需要紧急调整

---

## 🔍 根本原因诊断流程

### **第一步：检查数据集质量**

```python
# 手动验证掩码值
from PIL import Image
import numpy as np

# 检查几个掩码文件
mask = Image.open('masks/sample.png')
mask_arr = np.array(mask)
print(f"掩码数据类型: {mask_arr.dtype}")
print(f"唯一值: {np.unique(mask_arr)}")
print(f"值范围: [{mask_arr.min()}, {mask_arr.max()}]")
print(f"前景像素比例: {(mask_arr > 127).sum() / mask_arr.size:.4f}")
```

**常见问题：**
- ❌ 掩码是 PNG 8-bit (0-255) 但代码按 [0,1] 归一化 → **掩码值错误**
- ❌ 掩码全为背景（全0）或全为前景（全255） → **标注有误**
- ❌ 前景像素 < 2% → **严重类不平衡**
- ❌ 掩码边界模糊（灰度值50-200混杂） → **标注质量差**

**解决方案：**
```python
# 如果掩码是 0-255，需要在数据加载时归一化到 [0,1]
# 在 OCTADataset.__getitem__ 中：
mask_tensor = torch.from_numpy(mask_arr).float() / 255.0
```

---

### **第二步：检查Loss函数配置**

当前损失函数：`0.3 * BCE + 0.7 * (1 - Dice)`

**如果 Loss 卡在 0.6：**

| 情况 | BCE值 | Dice值 | 原因 | 解决方案 |
|-----|-------|--------|------|--------|
| BCE ≈ 0.3, Dice ≈ 0.43 | 低 | 高 | Dice项主导，模型未学会形状 | **降低 Dice 权重到 0.5** |
| BCE ≈ 0.6, Dice ≈ 0 | 高 | 低 | BCE 项主导，像素级分类未稳定 | **提高学习率或增加 epoch** |
| BCE ≈ 0.3, Dice ≈ 0.3 | 都低 | 都低 | 掩码质量差或模型不适配 | **检查掩码质量** |

**调整 Loss 权重：**
```python
# 在 train_controller.py 中修改路由参数（如果前端能传递）
# 或直接在 train_service.py 中修改 DiceBCELoss 权重
# 当前: self.dice_weight = 0.7
# 改为: self.dice_weight = 0.5  # 降低 Dice 权重
```

---

### **第三步：检查学习率**

当前配置：`初始 lr=0.0001, 每 10 epoch 衰减到 80%`

**如果观察到：**

| 现象 | 判断 | 解决方案 |
|-----|------|---------|
| Loss 第 1-3 epoch 下降快，之后平缓 | lr 过小 | **增加到 0.001 或 0.0005** |
| Loss 在 epoch 5 跳跃（上下抖动） | lr 过大 | **减小到 5e-5** |
| Loss 一直在 0.6 徘徊，无明显趋势 | lr 可能过小 | **试试 0.0005** |
| 学习率调度在 epoch 10 之前就开始衰减 | 调度不当 | **改为 step_size=20** |

**修改学习率：**
```python
# 在 train_service.py train_unet() 函数中
optimizer = optim.Adam(
    model.parameters(), 
    lr=0.0005,  # ← 改为 0.0005 尝试
    weight_decay=0
)

scheduler = optim.lr_scheduler.StepLR(
    optimizer, 
    step_size=20,  # ← 改为 20 epoch，给更多时间
    gamma=0.8
)
```

---

### **第四步：检查模型架构**

当前使用 `UNet_Transformer`（31M 参数）

**如果数据集很小（<50张）：**
- ❌ UNet_Transformer 太大，易过拟合
- ✅ 应该用参数更少的模型

**回退到原始 UNet：**
```python
# 在 train_service.py 中（约 371 行）
# 改为：
model = UNet(in_channels=3, out_channels=1).to(device)
```

---

### **第五步：检查数据增强**

当前增强：`hflip(50%), vflip(50%), rotation(±10°), scale(0.8-1.2), Gaussian noise(0.01)`

**如果 Loss 卡住且数据集小：**
- ❌ 增强过度，可能导致模型学习不稳定
- ✅ 减弱或禁用某些增强

```python
# 在 OCTADataset 中注释掉某些增强
# 例如，禁用 scale 增强（最激进的）
# 或降低旋转角度到 ±5°
```

---

## 📋 完整诊断检查清单

```
☐ 1. 掩码质量检查
     - [ ] 掩码值是 0-1 还是 0-255？
     - [ ] 前景像素比例是否在 5%-50% 范围内？
     - [ ] 掩码边界是否清晰（不是模糊的灰度）？

☐ 2. Loss 函数分量诊断
     - [ ] 记录 BCE 和 Dice 各自的值
     - [ ] 哪个分量在主导 loss？
     - [ ] 是否需要调整权重比例？

☐ 3. 学习率诊断
     - [ ] loss 在前几个 epoch 是否有明显下降？
     - [ ] 之后是平缓下降还是完全停滞？
     - [ ] 当前学习率是否太小？

☐ 4. 梯度流诊断
     - [ ] 是否出现 "梯度消失" 警告？
     - [ ] 深层梯度是否远小于浅层？
     - [ ] 是否需要移除某些 BN 层或改激活函数？

☐ 5. 数据集诊断
     - [ ] 数据集大小是否足够（建议 >100 张）？
     - [ ] 数据增强是否太激进？
     - [ ] 是否存在图像/掩码不对齐的情况？

☐ 6. 模型诊断
     - [ ] 当前模型是否过大（相对于数据集）？
     - [ ] 是否应该尝试更小的模型？
     - [ ] Xavier 初始化是否已应用？
```

---

## 🎯 快速修复方案（按优先级）

### **最可能的原因（概率 70%）：掩码质量**
```bash
# 操作：检查掩码
1. 查看掩码是否为正确的二值图像（只有 0 和 255）
2. 检查掩码和原图是否对齐
3. 确认掩码不是模糊的灰度图
```

### **第二可能（概率 15%）：学习率过小**
```python
# 改动：提高学习率
lr = 0.0005  # 从 0.0001 改为 0.0005（增加 5 倍）
step_size = 20  # 从 10 改为 20（给更多时间）
```

### **第三可能（概率 10%）：Loss 权重不合理**
```python
# 改动：调整 Loss 权重
# 在 DiceBCELoss 中
self.dice_weight = 0.5  # 从 0.7 改为 0.5
# 这样变成 0.3 * BCE + 0.5 * (1-Dice)
```

### **第四可能（概率 5%）：模型过大**
```python
# 改动：使用更小的模型
model = UNet(...)  # 改为原始 UNet（8.5M vs 31M 参数）
```

---

## 📞 还需要帮助？

如果按照上述步骤仍无法解决，请收集以下信息：

1. **数据集统计**：
   - 图像数量
   - 掩码前景比例
   - 数据增强情况

2. **Loss 日志**（训练前 20 个 epoch）：
   - 每个 epoch 的 Train Loss、BCE、Dice
   - 梯度范数情况

3. **诊断输出**（第 5、10、15 个 epoch）：
   - 模型输出范围
   - Loss 分量
   - 是否进入平台期

有了这些信息，能精准定位问题并给出解决方案。

---

**最后更新：2026年1月22日**
